{
  "dashboard": {
    "id": null,
    "title": "Nexten ML/AI Performance Dashboard",
    "tags": ["nexten", "ml", "ai", "performance", "fastapi"],
    "style": "dark",
    "timezone": "browser",
    "refresh": "30s",
    "schemaVersion": 30,
    "version": 1,
    "panels": [
      {
        "id": 1,
        "title": "API Request Rate (requests/sec)",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "targets": [
          {
            "expr": "sum by (service) (rate(fastapi_requests_total[5m]))",
            "legendFormat": "{{service}}",
            "refId": "A"
          }
        ],
        "yAxes": [
          {
            "label": "Requests/sec",
            "min": 0
          }
        ],
        "legend": {
          "alignAsTable": true,
          "rightSide": true,
          "values": true,
          "current": true,
          "max": true,
          "min": true
        },
        "tooltip": {
          "shared": true,
          "sort": 2,
          "value_type": "individual"
        }
      },
      {
        "id": 2,
        "title": "API Response Time (95th percentile)",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum by (service, le) (rate(fastapi_request_duration_seconds_bucket[5m])))",
            "legendFormat": "{{service}} - 95th",
            "refId": "A"
          },
          {
            "expr": "histogram_quantile(0.50, sum by (service, le) (rate(fastapi_request_duration_seconds_bucket[5m])))",
            "legendFormat": "{{service}} - median",
            "refId": "B"
          }
        ],
        "yAxes": [
          {
            "label": "Seconds",
            "min": 0
          }
        ],
        "alert": {
          "frequency": "10s",
          "conditions": [
            {
              "evaluator": {
                "params": [5],
                "type": "gt"
              },
              "operator": {
                "type": "and"
              },
              "query": {
                "params": ["A", "5m", "now"]
              },
              "reducer": {
                "params": [],
                "type": "last"
              },
              "type": "query"
            }
          ]
        }
      },
      {
        "id": 3,
        "title": "ML Inference Duration",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
        "targets": [
          {
            "expr": "histogram_quantile(0.50, sum by (model_type, service, le) (rate(ml_inference_duration_seconds_bucket[5m])))",
            "legendFormat": "{{model_type}} ({{service}}) - median",
            "refId": "A"
          },
          {
            "expr": "histogram_quantile(0.95, sum by (model_type, service, le) (rate(ml_inference_duration_seconds_bucket[5m])))",
            "legendFormat": "{{model_type}} ({{service}}) - 95th",
            "refId": "B"
          }
        ],
        "yAxes": [
          {
            "label": "Seconds",
            "min": 0
          }
        ]
      },
      {
        "id": 4,
        "title": "ML Inference Success Rate (%)",
        "type": "stat",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8},
        "targets": [
          {
            "expr": "sum by (model_type) (rate(ml_inference_total{status=\"success\"}[5m])) / sum by (model_type) (rate(ml_inference_total[5m])) * 100",
            "legendFormat": "{{model_type}}",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "min": 0,
            "max": 100,
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 90},
                {"color": "green", "value": 95}
              ]
            }
          }
        }
      },
      {
        "id": 5,
        "title": "Error Rate by Service (%)",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 16},
        "targets": [
          {
            "expr": "sum by (service) (rate(fastapi_requests_total{status_code!~\"2..\"}[5m])) / sum by (service) (rate(fastapi_requests_total[5m])) * 100",
            "legendFormat": "{{service}}",
            "refId": "A"
          }
        ],
        "yAxes": [
          {
            "label": "Error Rate (%)",
            "min": 0,
            "max": 100
          }
        ],
        "alert": {
          "frequency": "10s",
          "conditions": [
            {
              "evaluator": {
                "params": [5],
                "type": "gt"
              },
              "operator": {
                "type": "and"
              },
              "query": {
                "params": ["A", "5m", "now"]
              },
              "reducer": {
                "params": [],
                "type": "last"
              },
              "type": "query"
            }
          ]
        }
      },
      {
        "id": 6,
        "title": "Active Requests",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 16},
        "targets": [
          {
            "expr": "sum by (service) (fastapi_requests_in_progress)",
            "legendFormat": "{{service}}",
            "refId": "A"
          }
        ],
        "yAxes": [
          {
            "label": "Active Requests",
            "min": 0
          }
        ]
      },
      {
        "id": 7,
        "title": "Parsing Accuracy Distribution",
        "type": "heatmap",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 24},
        "targets": [
          {
            "expr": "sum by (parser_type, le) (rate(parsing_accuracy_score_bucket[5m]))",
            "legendFormat": "{{parser_type}}",
            "refId": "A"
          }
        ],
        "heatmap": {
          "xBucketSize": null,
          "yBucketSize": null,
          "xBucketNumber": null,
          "yBucketNumber": null
        }
      },
      {
        "id": 8,
        "title": "Matching Score Distribution",
        "type": "histogram",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 24},
        "targets": [
          {
            "expr": "sum by (le) (rate(matching_score_distribution_bucket[5m]))",
            "legendFormat": "Score {{le}}",
            "refId": "A"
          }
        ]
      },
      {
        "id": 9,
        "title": "File Processing Size (95th percentile)",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 32},
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum by (file_type, service, le) (rate(file_processing_size_bytes_bucket[5m])))",
            "legendFormat": "{{file_type}} ({{service}})",
            "refId": "A"
          }
        ],
        "yAxes": [
          {
            "label": "Bytes",
            "min": 0
          }
        ]
      },
      {
        "id": 10,
        "title": "System Resources Overview",
        "type": "table",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 32},
        "targets": [
          {
            "expr": "up",
            "legendFormat": "{{job}}",
            "refId": "A",
            "instant": true
          },
          {
            "expr": "(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100",
            "legendFormat": "Memory %",
            "refId": "B",
            "instant": true
          },
          {
            "expr": "100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU %",
            "refId": "C",
            "instant": true
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "templating": {
      "list": [
        {
          "name": "service",
          "type": "query",
          "query": "label_values(fastapi_requests_total, service)",
          "refresh": 1,
          "includeAll": true,
          "multi": true
        },
        {
          "name": "model_type",
          "type": "query", 
          "query": "label_values(ml_inference_total, model_type)",
          "refresh": 1,
          "includeAll": true,
          "multi": true
        }
      ]
    }
  }
}