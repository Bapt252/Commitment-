"""
üöÄ SuperSmartMatch V2 - Job Parser Ultra v2.0
PROMPT 2: Parser d'offres d'emploi ultra-optimis√© avec streaming temps r√©el

Features:
- Extraction cibl√©e selon sp√©cifications PROMPT 2
- Streaming temps r√©el avec WebSocket
- Validation interactive et correction utilisateur
- Cache Redis intelligent
- Support multi-formats avec OCR
- Scoring de confiance par champ
"""

import asyncio
import json
import logging
import os
import sys
import time
import uuid
from typing import Dict, List, Optional, Any

import aiofiles
import aioredis
from fastapi import (
    FastAPI, File, UploadFile, HTTPException, 
    WebSocket, WebSocketDisconnect, Form, Depends
)
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
import hashlib

# Import des modules partag√©s
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
from shared.middleware.metrics import (
    PrometheusMiddleware, 
    metrics_endpoint,
    track_ml_inference,
    track_parsing_accuracy,
    track_file_processing
)

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Models sp√©cifiques Job Parser selon PROMPT 2
class JobParseProgressUpdate(BaseModel):
    task_id: str
    status: str = Field(..., regex="^(processing|completed|error)$")
    progress: float = Field(..., ge=0, le=100)
    confidence: float = Field(..., ge=0, le=1)
    current_step: str
    data: Optional[Dict] = None
    suggestions: List[str] = []
    fallback_required: bool = False

class JobRequirements(BaseModel):
    required_skills: List[str] = []
    preferred_skills: List[str] = []
    minimum_experience: Optional[str] = None
    education_level: Optional[str] = None
    languages: List[str] = []
    certifications: List[str] = []
    confidence_score: float = 0.0

class CompensationInfo(BaseModel):
    salary_min: Optional[float] = None
    salary_max: Optional[float] = None
    salary_currency: str = "EUR"
    salary_period: str = "annual"  # annual, monthly, hourly
    other_benefits: List[str] = []
    confidence_score: float = 0.0

class WorkConditions(BaseModel):
    contract_type: Optional[str] = None  # CDI, CDD, Stage, Freelance
    location: Optional[str] = None
    remote_work: Optional[str] = None  # None, Partial, Full
    work_schedule: Optional[str] = None
    start_date: Optional[str] = None
    confidence_score: float = 0.0

class CompanyInfo(BaseModel):
    name: Optional[str] = None
    sector: Optional[str] = None
    size: Optional[str] = None
    description: Optional[str] = None
    culture: List[str] = []
    values: List[str] = []
    confidence_score: float = 0.0

class JobDataExtracted(BaseModel):
    # Extraction cibl√©e selon PROMPT 2
    title: Optional[str] = None
    level: Optional[str] = None  # Junior, Senior, Lead, Manager
    requirements: JobRequirements
    compensation: CompensationInfo
    work_conditions: WorkConditions
    company_info: CompanyInfo
    description: Optional[str] = None
    responsibilities: List[str] = []
    advantages: List[str] = []
    application_process: Optional[str] = None
    overall_confidence: float = 0.0

class JobParseResponse(BaseModel):
    task_id: str
    status: str
    progress: float
    confidence: float
    data: Optional[JobDataExtracted] = None
    suggestions: List[str] = []
    fallback_required: bool = False
    metadata: Optional[Dict] = None

class JobValidationUpdate(BaseModel):
    task_id: str
    field_path: str
    new_value: Any
    confidence_override: Optional[float] = None

# WebSocket Manager pour Job Parser
class JobConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
    
    async def connect(self, websocket: WebSocket, task_id: str):
        await websocket.accept()
        self.active_connections[task_id] = websocket
        logger.info(f"Job Parser WebSocket connected for task {task_id}")
    
    def disconnect(self, task_id: str):
        if task_id in self.active_connections:
            del self.active_connections[task_id]
            logger.info(f"Job Parser WebSocket disconnected for task {task_id}")
    
    async def send_progress(self, task_id: str, update: JobParseProgressUpdate):
        if task_id in self.active_connections:
            try:
                await self.active_connections[task_id].send_text(
                    update.json()
                )
            except Exception as e:
                logger.error(f"Error sending Job Parser WebSocket update: {e}")
                self.disconnect(task_id)

# Application FastAPI
app = FastAPI(
    title="Job Parser Ultra v2.0",
    description="Parser d'offres d'emploi ultra-performant avec streaming temps r√©el",
    version="2.0.0",
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(PrometheusMiddleware, service_name="job-parser-ultra")

# Manager global pour Job Parser
job_manager = JobConnectionManager()

# Fonctions utilitaires
async def get_redis():
    redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
    return await aioredis.from_url(redis_url)

def generate_file_hash(content: bytes) -> str:
    """G√©n√®re un hash unique pour le contenu du fichier"""
    return hashlib.sha256(content).hexdigest()

def calculate_job_confidence_score(data: Dict) -> float:
    """Calcule un score de confiance bas√© sur les champs remplis pour un job"""
    total_fields = 0
    filled_fields = 0
    
    # Champs essentiels d'un job posting
    essential_fields = [
        'title', 'level', 'description'
    ]
    
    for field in essential_fields:
        total_fields += 1
        if data.get(field):
            filled_fields += 1
    
    # Sections importantes
    sections = [
        'requirements', 'compensation', 'work_conditions', 
        'company_info', 'responsibilities'
    ]
    
    for section in sections:
        total_fields += 1
        section_data = data.get(section, {})
        if section_data and any(section_data.values()):
            filled_fields += 1
    
    return filled_fields / total_fields if total_fields > 0 else 0.0

async def simulate_job_parsing(file_content: bytes, filename: str, task_id: str) -> JobDataExtracted:
    """
    Simule le parsing avanc√© d'une offre d'emploi avec √©tapes progressives
    Dans un vrai syst√®me, ceci ferait appel √† OpenAI GPT-4, NLP avanc√©, etc.
    """
    
    # √âtape 1: Extraction du texte brut
    await job_manager.send_progress(task_id, JobParseProgressUpdate(
        task_id=task_id,
        status="processing",
        progress=15,
        confidence=0.1,
        current_step="Extraction du texte de l'offre d'emploi..."
    ))
    await asyncio.sleep(0.3)
    
    # √âtape 2: D√©tection du titre et niveau
    await job_manager.send_progress(task_id, JobParseProgressUpdate(
        task_id=task_id,
        status="processing", 
        progress=30,
        confidence=0.3,
        current_step="Identification du titre et niveau de poste..."
    ))
    await asyncio.sleep(0.4)
    
    # √âtape 3: Analyse des comp√©tences requises
    await job_manager.send_progress(task_id, JobParseProgressUpdate(
        task_id=task_id,
        status="processing",
        progress=50,
        confidence=0.5,
        current_step="Extraction des comp√©tences et exigences..."
    ))
    await asyncio.sleep(0.3)
    
    # √âtape 4: D√©tection des conditions de travail et r√©mun√©ration
    await job_manager.send_progress(task_id, JobParseProgressUpdate(
        task_id=task_id,
        status="processing",
        progress=75,
        confidence=0.7,
        current_step="Analyse des conditions de travail et r√©mun√©ration..."
    ))
    await asyncio.sleep(0.4)
    
    # √âtape 5: Extraction des informations entreprise
    await job_manager.send_progress(task_id, JobParseProgressUpdate(
        task_id=task_id,
        status="processing",
        progress=90,
        confidence=0.9,
        current_step="Extraction des informations sur l'entreprise..."
    ))
    await asyncio.sleep(0.2)
    
    # Simulation de donn√©es extraites (√† remplacer par un vrai parser IA)
    extracted_data = JobDataExtracted(
        title="D√©veloppeur Full Stack Senior",
        level="Senior",
        description="Nous recherchons un d√©veloppeur full stack exp√©riment√© pour rejoindre notre √©quipe dynamique. Vous travaillerez sur des projets innovants utilisant les derni√®res technologies.",
        requirements=JobRequirements(
            required_skills=[
                "Python", "JavaScript", "React", "FastAPI", "PostgreSQL", 
                "Docker", "Git", "REST APIs"
            ],
            preferred_skills=[
                "Machine Learning", "AWS", "Kubernetes", "TypeScript", "Redis"
            ],
            minimum_experience="5 ans",
            education_level="Bac+5 ou √©quivalent",
            languages=["Fran√ßais (courant)", "Anglais (professionnel)"],
            certifications=["AWS Solutions Architect (un plus)"],
            confidence_score=0.94
        ),
        compensation=CompensationInfo(
            salary_min=55000,
            salary_max=75000,
            salary_currency="EUR",
            salary_period="annual",
            other_benefits=[
                "T√©l√©travail partiel", "RTT", "Mutuelle", 
                "Tickets restaurant", "Formations"
            ],
            confidence_score=0.87
        ),
        work_conditions=WorkConditions(
            contract_type="CDI",
            location="Paris, France",
            remote_work="Partial",
            work_schedule="35h/semaine",
            start_date="D√®s que possible",
            confidence_score=0.92
        ),
        company_info=CompanyInfo(
            name="TechInnovate",
            sector="Technologie / FinTech",
            size="50-200 employ√©s",
            description="Startup innovante sp√©cialis√©e dans les solutions FinTech B2B",
            culture=["Agilit√©", "Innovation", "Esprit d'√©quipe"],
            values=["Excellence technique", "Impact client", "Croissance durable"],
            confidence_score=0.89
        ),
        responsibilities=[
            "D√©velopper et maintenir des applications web full stack",
            "Collaborer avec l'√©quipe produit pour d√©finir les sp√©cifications",
            "Optimiser les performances et la scalabilit√©",
            "Participer aux code reviews et √† l'am√©lioration continue",
            "Mentorer les d√©veloppeurs junior"
        ],
        advantages=[
            "T√©l√©travail hybride 2-3 jours/semaine",
            "Budget formation 2000‚Ç¨/an",
            "√âquipement derni√®re g√©n√©ration",
            "√âquipe internationale",
            "Projets √† fort impact"
        ],
        application_process="Candidature via notre site web avec CV et lettre de motivation. Entretien technique puis RH."
    )
    
    # Calcul du score de confiance global
    extracted_data.overall_confidence = calculate_job_confidence_score(extracted_data.dict())
    
    return extracted_data

# Endpoints API v2 pour Job Parser
@app.get("/")
async def root():
    return {
        "service": "Job Parser Ultra v2.0",
        "version": "2.0.0",
        "features": [
            "Extraction cibl√©e selon PROMPT 2",
            "WebSocket streaming temps r√©el",
            "Validation interactive",
            "Cache intelligent Redis",
            "Support multi-formats",
            "OCR pour documents scann√©s",
            "Scoring de confiance par champ"
        ],
        "extraction_targets": [
            "Titre du poste et niveau",
            "Comp√©tences requises et souhait√©es", 
            "Exp√©rience minimale exig√©e",
            "Localisation et t√©l√©travail",
            "Fourchette salariale",
            "Type de contrat (CDI/CDD/Stage)",
            "Avantages et culture entreprise"
        ]
    }

@app.get("/health")
async def health_check():
    return {"status": "ok", "service": "job-parser-ultra", "timestamp": time.time()}

@app.get("/metrics")
async def get_metrics():
    return await metrics_endpoint()

@app.post("/v2/parse/job/stream", response_model=JobParseResponse)
async def parse_job_stream(
    file: UploadFile = File(...),
    force_refresh: bool = Form(False)
):
    """
    Parse une offre d'emploi avec streaming temps r√©el via WebSocket
    
    Extraction cibl√©e selon PROMPT 2:
    - Titre du poste et niveau
    - Comp√©tences requises et souhait√©es
    - Exp√©rience minimale exig√©e
    - Localisation et t√©l√©travail
    - Fourchette salariale
    - Type de contrat (CDI/CDD/Stage)
    - Avantages et culture entreprise
    """
    
    # Validation du fichier
    if not file.filename:
        raise HTTPException(status_code=400, detail="Nom de fichier requis")
    
    file_extension = os.path.splitext(file.filename)[1].lower()
    allowed_extensions = ['.pdf', '.doc', '.docx', '.txt', '.html', '.jpg', '.jpeg', '.png']
    
    if file_extension not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"Format non support√©. Formats accept√©s: {', '.join(allowed_extensions)}"
        )
    
    # V√©rification de la taille (5MB max pour les jobs selon docker-compose)
    content = await file.read()
    if len(content) > 5 * 1024 * 1024:  # 5MB
        raise HTTPException(status_code=400, detail="Fichier trop volumineux (max 5MB)")
    
    # G√©n√©ration d'un task_id unique
    task_id = str(uuid.uuid4())
    
    # V√©rification du cache Redis
    redis = await get_redis()
    file_hash = generate_file_hash(content)
    cache_key = f"job_parsing:{file_hash}"
    
    if not force_refresh:
        cached_result = await redis.get(cache_key)
        if cached_result:
            logger.info(f"R√©sultat job trouv√© en cache pour {file_hash}")
            cached_data = json.loads(cached_result)
            return JobParseResponse(
                task_id=task_id,
                status="completed",
                progress=100,
                confidence=cached_data.get("overall_confidence", 0.9),
                data=JobDataExtracted(**cached_data),
                metadata={
                    "from_cache": True,
                    "file_hash": file_hash,
                    "timestamp": time.time()
                }
            )
    
    # Tracking des m√©triques
    track_file_processing(file_extension[1:], "job-parser-ultra", len(content))
    
    # Retour imm√©diat avec task_id - le parsing se fera via WebSocket
    return JobParseResponse(
        task_id=task_id,
        status="processing",
        progress=0,
        confidence=0.0,
        suggestions=[
            "Connectez-vous au WebSocket /v2/parse/job/status/{task_id} pour le suivi en temps r√©el",
            "Les offres structur√©es donnent de meilleurs r√©sultats",
            "Les documents PDF natives sont mieux analys√©s que les scans"
        ],
        metadata={
            "file_name": file.filename,
            "file_size": len(content),
            "file_type": file_extension[1:],
            "cache_key": cache_key,
            "from_cache": False
        }
    )

@app.websocket("/v2/parse/job/status/{task_id}")
async def websocket_job_parse_status(websocket: WebSocket, task_id: str):
    """
    WebSocket pour le suivi temps r√©el du parsing d'offres d'emploi
    Feedback <500ms selon sp√©cifications PROMPT 2
    """
    await job_manager.connect(websocket, task_id)
    
    try:
        # Simulation du parsing avec envoi d'updates temps r√©el
        # Dans le vrai syst√®me, on r√©cup√©rerait le fichier depuis une queue
        
        # Pour la d√©mo, on simule avec un fichier factice
        fake_content = b"Offre d'emploi - D√©veloppeur Full Stack Senior..."
        
        start_time = time.time()
        extracted_data = await simulate_job_parsing(fake_content, "job_offer.pdf", task_id)
        
        # Mise en cache du r√©sultat
        redis = await get_redis()
        cache_key = f"job_parsing:{generate_file_hash(fake_content)}"
        await redis.set(
            cache_key, 
            json.dumps(extracted_data.dict()), 
            ex=3600  # Expire apr√®s 1 heure
        )
        
        # Envoi du r√©sultat final
        processing_time = time.time() - start_time
        
        await job_manager.send_progress(task_id, JobParseProgressUpdate(
            task_id=task_id,
            status="completed",
            progress=100,
            confidence=extracted_data.overall_confidence,
            current_step="Parsing de l'offre d'emploi termin√© avec succ√®s !",
            data=extracted_data.dict(),
            suggestions=[
                "V√©rifiez les informations extraites, en particulier la r√©mun√©ration",
                "Les champs avec une confiance <0.8 peuvent n√©cessiter une validation",
                f"Analyse compl√©t√©e en {processing_time:.2f}s",
                "Utilisez l'endpoint de correction pour ajuster si n√©cessaire"
            ]
        ))
        
        # Tracking m√©triques de succ√®s
        track_ml_inference("job-parser-ultra-v2", "job-parser-ultra", processing_time, success=True)
        track_parsing_accuracy("job", file_extension[1:] if 'file_extension' in locals() else "pdf", extracted_data.overall_confidence)
        
        # Garder la connexion ouverte pour les corrections potentielles
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            
            if message.get("type") == "ping":
                await websocket.send_text(json.dumps({"type": "pong"}))
            elif message.get("type") == "close":
                break
                
    except WebSocketDisconnect:
        job_manager.disconnect(task_id)
    except Exception as e:
        logger.error(f"Erreur WebSocket job parsing: {e}")
        if task_id in job_manager.active_connections:
            await job_manager.send_progress(task_id, JobParseProgressUpdate(
                task_id=task_id,
                status="error",
                progress=0,
                confidence=0.0,
                current_step=f"Erreur: {str(e)}",
                fallback_required=True,
                suggestions=[
                    "Une erreur est survenue pendant l'analyse de l'offre",
                    "V√©rifiez que le document contient bien une offre d'emploi",
                    "Essayez avec un format diff√©rent ou contactez le support"
                ]
            ))
        job_manager.disconnect(task_id)

@app.get("/v2/parse/job/validate/{task_id}")
async def get_job_validation_data(task_id: str):
    """
    R√©cup√®re les donn√©es d'un parsing d'offre pour validation interactive
    """
    redis = await get_redis()
    result_key = f"job_result:{task_id}"
    
    cached_result = await redis.get(result_key)
    if not cached_result:
        raise HTTPException(status_code=404, detail="R√©sultat de parsing d'offre non trouv√©")
    
    return json.loads(cached_result)

@app.put("/v2/parse/job/corrections/{task_id}")
async def apply_job_corrections(task_id: str, corrections: List[JobValidationUpdate]):
    """
    Applique les corrections utilisateur au parsing d'offre d'emploi
    Permet la validation interactive selon PROMPT 2
    """
    redis = await get_redis()
    result_key = f"job_result:{task_id}"
    
    cached_result = await redis.get(result_key)
    if not cached_result:
        raise HTTPException(status_code=404, detail="R√©sultat de parsing d'offre non trouv√©")
    
    data = json.loads(cached_result)
    
    # Application des corrections
    for correction in corrections:
        field_parts = correction.field_path.split('.')
        current = data
        
        # Navigation vers le champ √† corriger
        for part in field_parts[:-1]:
            if isinstance(current, list) and part.isdigit():
                current = current[int(part)]
            else:
                current = current.get(part, {})
        
        # Application de la correction
        field_name = field_parts[-1]
        if isinstance(current, dict):
            current[field_name] = correction.new_value
            
            # Mise √† jour du score de confiance si fourni
            if correction.confidence_override:
                confidence_field = f"{field_name}_confidence_score"
                if confidence_field in current:
                    current[confidence_field] = correction.confidence_override
    
    # Recalcul du score de confiance global
    data["overall_confidence"] = calculate_job_confidence_score(data)
    
    # Sauvegarde des corrections
    await redis.set(result_key, json.dumps(data), ex=3600)
    
    logger.info(f"Corrections d'offre appliqu√©es pour task {task_id}: {len(corrections)} champs modifi√©s")
    
    return {
        "task_id": task_id,
        "corrections_applied": len(corrections),
        "new_confidence": data["overall_confidence"],
        "message": "Corrections appliqu√©es avec succ√®s sur l'offre d'emploi"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=int(os.getenv("PORT", 5053)),
        reload=os.getenv("ENVIRONMENT") == "development"
    )
