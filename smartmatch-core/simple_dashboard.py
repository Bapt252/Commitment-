#!/usr/bin/env python3
"""
Dashboard Session 5 - Version SimplifiÃ©e
========================================

Dashboard Streamlit simple pour la dÃ©monstration de Session 5
avec visualisations interactives et contrÃ´les d'administration.

Features:
- Vue d'ensemble systÃ¨me en temps rÃ©el
- Gestion des modÃ¨les ML
- Monitoring A/B tests
- MÃ©triques de performance
- ContrÃ´les administrateur

Usage: streamlit run simple_dashboard.py

Author: AI Assistant & Bapt252
Session: 5 - ML Optimization Intelligence
"""
import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import time

# Configuration de la page
st.set_page_config(
    page_title="Session 5 - ML Optimization Dashboard",
    page_icon="ğŸš€",
    layout="wide"
)

# Titre principal
st.title("ğŸš€ Session 5 - SystÃ¨me d'Optimisation ML")

# Sidebar
with st.sidebar:
    st.header("ğŸ›ï¸ ContrÃ´les")
    auto_refresh = st.checkbox("Auto-refresh (5s)", value=False)
    
    st.markdown("---")
    st.subheader("ğŸ“Š Statut SystÃ¨me")
    
    # MÃ©triques systÃ¨me mockÃ©es
    col1, col2 = st.columns(2)
    with col1:
        st.metric("CPU", "65%", "â†‘ 5%")
        st.metric("ModÃ¨les", "3", "â†‘ 1")
    with col2:
        st.metric("MÃ©moire", "45%", "â†“ 2%")
        st.metric("Tests A/B", "2", "â†’ 0")

# Corps principal
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ˆ Overview", "ğŸ¤– ModÃ¨les", "ğŸ§ª A/B Tests", "ğŸ“Š Monitoring"])

with tab1:
    st.header("Vue d'ensemble du systÃ¨me")
    
    # MÃ©triques principales
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Performance", "94.5%", "â†‘ 2.1%")
    with col2:
        st.metric("Latence", "120ms", "â†“ 15ms")
    with col3:
        st.metric("Uptime", "99.9%", "â†’ 0%")
    with col4:
        st.metric("Erreurs", "0.1%", "â†“ 0.05%")
    
    # Graphique de performance
    dates = pd.date_range('2024-05-01', periods=30, freq='D')
    performance = np.random.normal(0.94, 0.02, 30)
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=dates,
        y=performance,
        mode='lines+markers',
        name='Performance',
        line=dict(color='#1f77b4', width=3)
    ))
    fig.update_layout(
        title="Performance du SystÃ¨me (30 derniers jours)",
        xaxis_title="Date",
        yaxis_title="Performance (%)",
        height=400
    )
    st.plotly_chart(fig, use_container_width=True)

with tab2:
    st.header("Gestion des ModÃ¨les")
    
    # Table des modÃ¨les
    models_data = {
        'Nom': ['Enhanced Skills Matcher v1.2', 'TF-IDF Baseline', 'Hybrid Model'],
        'Version': ['1.2.0', '1.0.0', '1.1.0'],
        'Statut': ['ğŸŸ¢ DÃ©ployÃ©', 'ğŸŸ¡ Staging', 'ğŸ”µ Training'],
        'Performance': [94.5, 89.2, 92.1],
        'DÃ©ployÃ©': ['2024-05-15', '2024-05-10', '-']
    }
    
    df = pd.DataFrame(models_data)
    st.dataframe(df, use_container_width=True)
    
    # Boutons de contrÃ´le
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("ğŸš€ DÃ©ployer Nouveau ModÃ¨le"):
            st.success("DÃ©ploiement initiÃ© pour Hybrid Model v1.1.0")
    with col2:
        if st.button("â†©ï¸ Rollback"):
            st.warning("Rollback vers Enhanced Skills Matcher v1.1")
    with col3:
        if st.button("ğŸ”„ Actualiser"):
            st.info("Statut des modÃ¨les actualisÃ©")

with tab3:
    st.header("Tests A/B en cours")
    
    # Configuration test A/B
    with st.expander("ğŸ¯ Nouveau Test A/B"):
        test_name = st.text_input("Nom du test", "Test Algorithme Matching")
        control_model = st.selectbox("ModÃ¨le ContrÃ´le", ['Enhanced Skills Matcher v1.2'])
        variant_model = st.selectbox("ModÃ¨le Variant", ['Hybrid Model v1.1'])
        traffic_split = st.slider("RÃ©partition trafic (%)", 10, 50, 20)
        
        if st.button("ğŸš€ Lancer Test"):
            st.success(f"Test A/B '{test_name}' lancÃ© avec {traffic_split}% de trafic")
    
    # RÃ©sultats tests existants
    st.subheader("ğŸ“Š RÃ©sultats actuels")
    
    # Mock data pour graphique A/B
    metrics = ['Precision', 'Recall', 'F1-Score']
    control = [0.89, 0.85, 0.87]
    variant = [0.91, 0.88, 0.89]
    
    fig = go.Figure()
    fig.add_trace(go.Bar(x=metrics, y=control, name='ContrÃ´le', marker_color='lightblue'))
    fig.add_trace(go.Bar(x=metrics, y=variant, name='Variant', marker_color='lightcoral'))
    
    fig.update_layout(
        title="Comparaison Performance: ContrÃ´le vs Variant",
        xaxis_title="MÃ©triques",
        yaxis_title="Score",
        barmode='group',
        height=400
    )
    st.plotly_chart(fig, use_container_width=True)

with tab4:
    st.header("Monitoring SystÃ¨me")
    
    # Graphiques de monitoring
    col1, col2 = st.columns(2)
    
    with col1:
        # CPU/Memory over time
        hours = list(range(24))
        cpu_usage = np.random.normal(60, 15, 24)
        memory_usage = np.random.normal(50, 10, 24)
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=hours, y=cpu_usage, name='CPU %', line=dict(color='red')))
        fig.add_trace(go.Scatter(x=hours, y=memory_usage, name='Memory %', line=dict(color='blue')))
        
        fig.update_layout(
            title="Utilisation Ressources (24h)",
            xaxis_title="Heure",
            yaxis_title="Utilisation (%)",
            height=400
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        # Drift detection
        days = list(range(1, 31))
        drift_scores = np.random.exponential(0.05, 30)
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=days, 
            y=drift_scores, 
            mode='lines+markers',
            name='Drift Score',
            line=dict(color='green')
        ))
        fig.add_hline(y=0.1, line_dash="dash", line_color="red", annotation_text="Seuil d'alerte")
        
        fig.update_layout(
            title="DÃ©tection de DÃ©rive (30 jours)",
            xaxis_title="Jour",
            yaxis_title="Score de DÃ©rive",
            height=400
        )
        st.plotly_chart(fig, use_container_width=True)

# Section Configuration SystÃ¨me
st.markdown("---")
with st.expander("âš™ï¸ Configuration SystÃ¨me"):
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Pipeline")
        st.text("Training Interval: 30 min")
        st.text("A/B Test Duration: 12h")
        st.text("Drift Check: 5 min")
        
    with col2:
        st.subheader("Admin Interface")
        st.text("Dashboard Port: 8501")
        st.text("API Port: 8080")
        st.text("Auth: Disabled (Demo)")

# Alertes systÃ¨me
if st.button("ğŸ”” DÃ©clencher Alerte Test"):
    st.error("âš ï¸ Alert Test: DÃ©rive dÃ©tectÃ©e sur le modÃ¨le principal (Score: 0.15)")
    st.warning("ğŸ”„ RÃ©-entraÃ®nement automatique initiÃ©...")

# Auto-refresh
if auto_refresh:
    time.sleep(5)
    st.rerun()

# Footer
st.markdown("---")
st.markdown("ğŸ¯ **Session 5: ML Optimization Intelligence** - SystÃ¨me opÃ©rationnel")
st.markdown("ğŸ“Š Dashboard: `streamlit run simple_dashboard.py`")
st.markdown("ğŸ”§ Demo: `python demo_session5_integration_fixed.py --create-config`")

# Instructions d'utilisation
with st.sidebar:
    st.markdown("---")
    st.subheader("ğŸ“ Instructions")
    st.markdown("""
    **Pour utiliser Session 5:**
    
    1. **Dashboard**: DÃ©jÃ  actif ! ğŸ‰
    
    2. **Demo Python**:
    ```bash
    python demo_session5_integration_fixed.py --create-config
    python demo_session5_integration_fixed.py --config session5_demo_config.json
    ```
    
    3. **API Admin**: http://localhost:8080
    
    4. **Composants Session 5**:
    - âœ… Pipeline orchestration
    - âœ… Admin dashboard
    - âœ… Model controller
    - âœ… Optimization system
    """)
