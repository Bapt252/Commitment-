#!/usr/bin/env python3
"""
Test du refactoring SmartMatcher - Session 3
-------------------------------------------
Script de test complet pour valider le nouveau moteur SmartMatchEngine
et comparer avec l'ancien SmartMatcher.

Tests inclus:
- Tests unitaires des nouveaux matchers
- Tests d'intégration du moteur principal  
- Tests de performance et comparaison
- Tests de compatibilité avec l'ancienne interface
- Validation des insights et scoring
"""

import asyncio
import time
import sys
import os
import logging
from typing import Dict, List, Any
from datetime import datetime

# Ajouter le chemin pour les imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    # Imports du nouveau système
    from engine import SmartMatchEngine, LegacySmartMatcher
    from core.models import Candidate, Job, Location, SalaryRange
    from core.config import SmartMatchConfig
    from matchers.skills_matcher import SkillsMatcher
    from matchers.location_matcher import LocationMatcher
    from matchers.experience_matcher import ExperienceMatcher
    
    # Import de l'ancien système pour comparaison
    from smartmatch import SmartMatcher as OldSmartMatcher
    
    NEW_ENGINE_AVAILABLE = True
except ImportError as e:
    logger.error(f\"Erreur import nouveau moteur: {e}\")\n    NEW_ENGINE_AVAILABLE = False\n    \n    # Fallback vers ancien système seulement\n    try:\n        from smartmatch import SmartMatcher as OldSmartMatcher\n        OLD_ENGINE_AVAILABLE = True\n    except ImportError:\n        logger.error(\"Ancien moteur également indisponible\")\n        OLD_ENGINE_AVAILABLE = False\n\n\nclass SmartMatchTester:\n    \"\"\"\n    Testeur complet pour valider le refactoring du SmartMatcher.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialise le testeur avec les données de test.\"\"\"\n        self.test_data = self._load_test_data()\n        self.results = {\n            \"tests_run\": 0,\n            \"tests_passed\": 0,\n            \"tests_failed\": 0,\n            \"performance_metrics\": {},\n            \"compatibility_results\": {},\n            \"detailed_results\": []\n        }\n    \n    def _load_test_data(self) -> Dict[str, List[Dict[str, Any]]]:\n        \"\"\"Charge les données de test pour les matching tests.\"\"\"\n        # Candidats de test\n        candidates = [\n            {\n                \"id\": \"c1\",\n                \"name\": \"Alice Développeuse\",\n                \"skills\": [\"Python\", \"Django\", \"React\", \"PostgreSQL\", \"Docker\"],\n                \"location\": {\"latitude\": 48.8566, \"longitude\": 2.3522},  # Paris\n                \"years_of_experience\": 5,\n                \"education_level\": \"master\",\n                \"seniority_level\": \"senior\",\n                \"preferences\": {\n                    \"remote_work\": True,\n                    \"salary_expectation\": 65000,\n                    \"job_type\": \"full_time\",\n                    \"industry\": \"tech\"\n                },\n                \"experience\": [\n                    {\n                        \"position_title\": \"Senior Developer\",\n                        \"company\": \"TechCorp\",\n                        \"duration_years\": 3,\n                        \"technologies\": [\"Python\", \"Django\", \"React\"],\n                        \"start_date\": datetime(2021, 1, 1),\n                        \"end_date\": datetime(2024, 1, 1)\n                    },\n                    {\n                        \"position_title\": \"Lead Developer\",\n                        \"company\": \"StartupXYZ\", \n                        \"duration_years\": 2,\n                        \"technologies\": [\"Python\", \"FastAPI\"],\n                        \"team_size\": 5,\n                        \"start_date\": datetime(2019, 1, 1),\n                        \"end_date\": datetime(2021, 1, 1)\n                    }\n                ]\n            },\n            {\n                \"id\": \"c2\",\n                \"name\": \"Bob Architecture\",\n                \"skills\": [\"Java\", \"Spring\", \"Microservices\", \"Kubernetes\", \"AWS\"],\n                \"location\": {\"latitude\": 45.7640, \"longitude\": 4.8357},  # Lyon\n                \"years_of_experience\": 8,\n                \"education_level\": \"bachelor\",\n                \"seniority_level\": \"principal\",\n                \"preferences\": {\n                    \"remote_work\": False,\n                    \"salary_expectation\": 85000,\n                    \"job_type\": \"full_time\",\n                    \"industry\": \"finance\"\n                },\n                \"experience\": [\n                    {\n                        \"position_title\": \"Principal Architect\",\n                        \"company\": \"BankCorp\",\n                        \"duration_years\": 4,\n                        \"technologies\": [\"Java\", \"Spring\", \"Microservices\"],\n                        \"start_date\": datetime(2020, 1, 1),\n                        \"end_date\": None  # Poste actuel\n                    },\n                    {\n                        \"position_title\": \"Senior Architect\",\n                        \"company\": \"FinTech Inc\",\n                        \"duration_years\": 4,\n                        \"technologies\": [\"Java\", \"Spring Boot\"],\n                        \"team_size\": 10,\n                        \"start_date\": datetime(2016, 1, 1),\n                        \"end_date\": datetime(2020, 1, 1)\n                    }\n                ]\n            }\n        ]\n        \n        # Offres d'emploi de test\n        jobs = [\n            {\n                \"id\": \"j1\",\n                \"title\": \"Senior Python Developer\",\n                \"description\": \"Développeur Python expérimenté pour projet IA\",\n                \"required_skills\": [\"Python\", \"Django\", \"PostgreSQL\"],\n                \"preferred_skills\": [\"React\", \"Docker\", \"AI/ML\"],\n                \"location\": {\"latitude\": 48.8847, \"longitude\": 2.2967},  # Levallois\n                \"requirements\": {\n                    \"experience\": {\n                        \"min_years\": 4,\n                        \"max_years\": 8\n                    },\n                    \"education_level\": \"bachelor\",\n                    \"seniority_level\": \"senior\"\n                },\n                \"remote_policy\": {\n                    \"allowed\": True,\n                    \"policy\": \"hybrid\"\n                },\n                \"salary_range\": {\"min\": 55000, \"max\": 75000},\n                \"job_type\": \"full_time\",\n                \"industry\": \"tech\"\n            },\n            {\n                \"id\": \"j2\",\n                \"title\": \"Lead Java Architect\",\n                \"description\": \"Architecte Java pour système bancaire\",\n                \"required_skills\": [\"Java\", \"Spring\", \"Microservices\"],\n                \"preferred_skills\": [\"Kubernetes\", \"AWS\", \"DevOps\"],\n                \"location\": {\"latitude\": 48.8566, \"longitude\": 2.3522},  # Paris\n                \"requirements\": {\n                    \"experience\": {\n                        \"min_years\": 6,\n                        \"max_years\": 12,\n                        \"specific_areas\": [\n                            {\"name\": \"java\", \"min_years\": 5},\n                            {\"name\": \"microservices\", \"min_years\": 3}\n                        ]\n                    },\n                    \"education_level\": \"master\",\n                    \"seniority_level\": \"lead\",\n                    \"requires_leadership\": True\n                },\n                \"remote_policy\": {\n                    \"allowed\": False\n                },\n                \"salary_range\": {\"min\": 70000, \"max\": 90000},\n                \"job_type\": \"full_time\",\n                \"industry\": \"finance\"\n            },\n            {\n                \"id\": \"j3\",\n                \"title\": \"Junior Developer\",\n                \"description\": \"Poste junior pour développeur débutant\",\n                \"required_skills\": [\"JavaScript\", \"HTML\", \"CSS\"],\n                \"preferred_skills\": [\"React\", \"Node.js\"],\n                \"location\": {\"latitude\": 43.6043, \"longitude\": 1.4437},  # Toulouse\n                \"requirements\": {\n                    \"experience\": {\n                        \"min_years\": 0,\n                        \"max_years\": 3\n                    },\n                    \"education_level\": \"bachelor\",\n                    \"seniority_level\": \"junior\"\n                },\n                \"remote_policy\": {\n                    \"allowed\": True,\n                    \"policy\": \"full_remote\"\n                },\n                \"salary_range\": {\"min\": 35000, \"max\": 45000},\n                \"job_type\": \"full_time\",\n                \"industry\": \"tech\"\n            }\n        ]\n        \n        return {\"candidates\": candidates, \"jobs\": jobs}\n    \n    def _run_test(self, test_name: str, test_func) -> bool:\n        \"\"\"\n        Exécute un test individuel et enregistre le résultat.\n        \n        Args:\n            test_name: Nom du test\n            test_func: Fonction de test à exécuter\n            \n        Returns:\n            True si le test réussit, False sinon\n        \"\"\"\n        self.results[\"tests_run\"] += 1\n        \n        try:\n            start_time = time.time()\n            result = test_func()\n            execution_time = time.time() - start_time\n            \n            if result:\n                self.results[\"tests_passed\"] += 1\n                status = \"PASS\"\n                logger.info(f\"✓ {test_name} - {status} ({execution_time:.3f}s)\")\n            else:\n                self.results[\"tests_failed\"] += 1\n                status = \"FAIL\"\n                logger.error(f\"✗ {test_name} - {status} ({execution_time:.3f}s)\")\n            \n            self.results[\"detailed_results\"].append({\n                \"test_name\": test_name,\n                \"status\": status,\n                \"execution_time\": execution_time,\n                \"result\": result\n            })\n            \n            return result\n            \n        except Exception as e:\n            self.results[\"tests_failed\"] += 1\n            logger.error(f\"✗ {test_name} - ERROR: {str(e)}\")\n            \n            self.results[\"detailed_results\"].append({\n                \"test_name\": test_name,\n                \"status\": \"ERROR\",\n                \"execution_time\": 0,\n                \"error\": str(e)\n            })\n            \n            return False\n    \n    async def _run_async_test(self, test_name: str, test_func) -> bool:\n        \"\"\"\n        Exécute un test asynchrone et enregistre le résultat.\n        \n        Args:\n            test_name: Nom du test\n            test_func: Fonction de test asynchrone à exécuter\n            \n        Returns:\n            True si le test réussit, False sinon\n        \"\"\"\n        self.results[\"tests_run\"] += 1\n        \n        try:\n            start_time = time.time()\n            result = await test_func()\n            execution_time = time.time() - start_time\n            \n            if result:\n                self.results[\"tests_passed\"] += 1\n                status = \"PASS\"\n                logger.info(f\"✓ {test_name} - {status} ({execution_time:.3f}s)\")\n            else:\n                self.results[\"tests_failed\"] += 1\n                status = \"FAIL\"\n                logger.error(f\"✗ {test_name} - {status} ({execution_time:.3f}s)\")\n            \n            self.results[\"detailed_results\"].append({\n                \"test_name\": test_name,\n                \"status\": status,\n                \"execution_time\": execution_time,\n                \"result\": result\n            })\n            \n            return result\n            \n        except Exception as e:\n            self.results[\"tests_failed\"] += 1\n            logger.error(f\"✗ {test_name} - ERROR: {str(e)}\")\n            \n            self.results[\"detailed_results\"].append({\n                \"test_name\": test_name,\n                \"status\": \"ERROR\",\n                \"execution_time\": 0,\n                \"error\": str(e)\n            })\n            \n            return False\n    \n    def test_old_engine_basic(self) -> bool:\n        \"\"\"\n        Test de base de l'ancien moteur SmartMatcher.\n        \n        Returns:\n            True si le test réussit\n        \"\"\"\n        if not OLD_ENGINE_AVAILABLE:\n            logger.warning(\"Ancien moteur non disponible, test ignoré\")\n            return True\n        \n        try:\n            # Initialiser l'ancien matcher\n            old_matcher = OldSmartMatcher()\n            \n            # Tester avec des données simples\n            candidate = self.test_data[\"candidates\"][0]\n            job = self.test_data[\"jobs\"][0]\n            \n            # Convertir au format attendu par l'ancien matcher\n            old_candidate = {\n                \"id\": candidate[\"id\"],\n                \"skills\": candidate[\"skills\"],\n                \"location\": f\"{candidate['location']['latitude']},{candidate['location']['longitude']}\",\n                \"years_of_experience\": candidate[\"years_of_experience\"],\n                \"education_level\": candidate[\"education_level\"],\n                \"remote_work\": candidate[\"preferences\"][\"remote_work\"],\n                \"salary_expectation\": candidate[\"preferences\"][\"salary_expectation\"]\n            }\n            \n            old_job = {\n                \"id\": job[\"id\"],\n                \"required_skills\": job[\"required_skills\"],\n                \"preferred_skills\": job[\"preferred_skills\"],\n                \"location\": f\"{job['location']['latitude']},{job['location']['longitude']}\",\n                \"min_years_of_experience\": job[\"requirements\"][\"experience\"][\"min_years\"],\n                \"max_years_of_experience\": job[\"requirements\"][\"experience\"][\"max_years\"],\n                \"required_education\": job[\"requirements\"][\"education_level\"],\n                \"offers_remote\": job[\"remote_policy\"][\"allowed\"],\n                \"salary_range\": job[\"salary_range\"]\n            }\n            \n            # Calculer le matching\n            result = old_matcher.calculate_match(old_candidate, old_job)\n            \n            # Vérifications\n            assert \"overall_score\" in result\n            assert \"category_scores\" in result\n            assert \"insights\" in result\n            assert 0 <= result[\"overall_score\"] <= 1\n            \n            logger.info(f\"Ancien moteur - Score: {result['overall_score']:.2f}\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Erreur test ancien moteur: {str(e)}\")\n            return False\n    \n    async def test_new_engine_basic(self) -> bool:\n        \"\"\"\n        Test de base du nouveau moteur SmartMatchEngine.\n        \n        Returns:\n            True si le test réussit\n        \"\"\"\n        if not NEW_ENGINE_AVAILABLE:\n            logger.warning(\"Nouveau moteur non disponible, test ignoré\")\n            return True\n        \n        try:\n            # Initialiser le nouveau moteur\n            config = SmartMatchConfig()\n            engine = SmartMatchEngine(config=config)\n            \n            # Tester avec des données\n            candidate_data = self.test_data[\"candidates\"][0]\n            job_data = self.test_data[\"jobs\"][0]\n            \n            # Calculer le matching\n            result = await engine.calculate_match(candidate_data, job_data)\n            \n            # Vérifications\n            assert result.overall_score is not None\n            assert result.category_scores is not None\n            assert result.insights is not None\n            assert 0 <= result.overall_score <= 1\n            assert len(result.insights) > 0\n            \n            logger.info(f\"Nouveau moteur - Score: {result.overall_score:.2f}\")\n            logger.info(f\"Insights générés: {len(result.insights)}\")\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Erreur test nouveau moteur: {str(e)}\")\n            return False\n    \n    def test_legacy_compatibility(self) -> bool:\n        \"\"\"\n        Test de compatibilité avec l'interface legacy.\n        \n        Returns:\n            True si le test réussit\n        \"\"\"\n        if not NEW_ENGINE_AVAILABLE:\n            logger.warning(\"Nouveau moteur non disponible, test ignoré\")\n            return True\n        \n        try:\n            # Initialiser avec l'interface legacy\n            legacy_matcher = LegacySmartMatcher()\n            \n            # Utiliser les mêmes données que l'ancien test\n            candidate = {\n                \"id\": \"test_c1\",\n                \"skills\": [\"Python\", \"Django\"],\n                \"location\": \"48.8566,2.3522\",\n                \"years_of_experience\": 5,\n                \"education_level\": \"master\",\n                \"remote_work\": True,\n                \"salary_expectation\": 65000\n            }\n            \n            job = {\n                \"id\": \"test_j1\",\n                \"required_skills\": [\"Python\", \"Django\"],\n                \"location\": \"48.8847,2.2967\",\n                \"min_years_of_experience\": 3,\n                \"required_education\": \"bachelor\",\n                \"offers_remote\": True,\n                \"salary_range\": {\"min\": 55000, \"max\": 75000}\n            }\n            \n            # Calculer le matching\n            result = legacy_matcher.calculate_match(candidate, job)\n            \n            # Vérifications du format de sortie\n            assert isinstance(result, dict)\n            assert \"overall_score\" in result\n            assert \"category_scores\" in result\n            assert \"insights\" in result\n            assert 0 <= result[\"overall_score\"] <= 1\n            \n            logger.info(f\"Interface legacy - Score: {result['overall_score']:.2f}\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Erreur test compatibilité legacy: {str(e)}\")\n            return False\n    \n    async def test_matchers_individual(self) -> bool:\n        \"\"\"\n        Test des matchers individuels.\n        \n        Returns:\n            True si tous les tests réussissent\n        \"\"\"\n        if not NEW_ENGINE_AVAILABLE:\n            logger.warning(\"Nouveau moteur non disponible, test ignoré\")\n            return True\n        \n        try:\n            # Préparer les données de test\n            candidate_data = self.test_data[\"candidates\"][0]\n            job_data = self.test_data[\"jobs\"][0]\n            \n            candidate = Candidate.from_dict(candidate_data)\n            job = Job.from_dict(job_data)\n            \n            success = True\n            \n            # Test SkillsMatcher\n            try:\n                skills_matcher = SkillsMatcher()\n                skills_score = await skills_matcher.calculate_match(candidate, job)\n                assert 0 <= skills_score <= 1\n                logger.info(f\"SkillsMatcher - Score: {skills_score:.2f}\")\n            except Exception as e:\n                logger.error(f\"Erreur SkillsMatcher: {str(e)}\")\n                success = False\n            \n            # Test ExperienceMatcher\n            try:\n                experience_matcher = ExperienceMatcher()\n                experience_score = await experience_matcher.calculate_match(candidate, job)\n                assert 0 <= experience_score <= 1\n                logger.info(f\"ExperienceMatcher - Score: {experience_score:.2f}\")\n            except Exception as e:\n                logger.error(f\"Erreur ExperienceMatcher: {str(e)}\")\n                success = False\n            \n            # Test LocationMatcher (sans API key)\n            try:\n                location_matcher = LocationMatcher(api_key=None)\n                location_score = await location_matcher.calculate_match(candidate, job)\n                assert 0 <= location_score <= 1\n                logger.info(f\"LocationMatcher - Score: {location_score:.2f}\")\n            except Exception as e:\n                logger.error(f\"Erreur LocationMatcher: {str(e)}\")\n                success = False\n            \n            return success\n            \n        except Exception as e:\n            logger.error(f\"Erreur test matchers individuels: {str(e)}\")\n            return False\n    \n    async def test_performance_comparison(self) -> bool:\n        \"\"\"\n        Test de performance entre ancien et nouveau moteur.\n        \n        Returns:\n            True si le test réussit\n        \"\"\"\n        try:\n            candidates = self.test_data[\"candidates\"]\n            jobs = self.test_data[\"jobs\"]\n            \n            results = {\n                \"old_engine\": {\"total_time\": 0, \"avg_time\": 0, \"available\": OLD_ENGINE_AVAILABLE},\n                \"new_engine\": {\"total_time\": 0, \"avg_time\": 0, \"available\": NEW_ENGINE_AVAILABLE},\n                \"legacy_interface\": {\"total_time\": 0, \"avg_time\": 0, \"available\": NEW_ENGINE_AVAILABLE}\n            }\n            \n            num_tests = len(candidates) * len(jobs)\n            \n            # Test ancien moteur\n            if OLD_ENGINE_AVAILABLE:\n                old_matcher = OldSmartMatcher()\n                start_time = time.time()\n                \n                for candidate in candidates:\n                    for job in jobs:\n                        # Conversion format\n                        old_candidate = {\n                            \"id\": candidate[\"id\"],\n                            \"skills\": candidate[\"skills\"],\n                            \"location\": f\"{candidate['location']['latitude']},{candidate['location']['longitude']}\",\n                            \"years_of_experience\": candidate[\"years_of_experience\"],\n                            \"education_level\": candidate[\"education_level\"]\n                        }\n                        \n                        old_job = {\n                            \"id\": job[\"id\"],\n                            \"required_skills\": job[\"required_skills\"],\n                            \"location\": f\"{job['location']['latitude']},{job['location']['longitude']}\",\n                            \"min_years_of_experience\": job[\"requirements\"][\"experience\"][\"min_years\"]\n                        }\n                        \n                        old_matcher.calculate_match(old_candidate, old_job)\n                \n                results[\"old_engine\"][\"total_time\"] = time.time() - start_time\n                results[\"old_engine\"][\"avg_time\"] = results[\"old_engine\"][\"total_time\"] / num_tests\n            \n            # Test nouveau moteur\n            if NEW_ENGINE_AVAILABLE:\n                engine = SmartMatchEngine()\n                start_time = time.time()\n                \n                for candidate in candidates:\n                    for job in jobs:\n                        await engine.calculate_match(candidate, job)\n                \n                results[\"new_engine\"][\"total_time\"] = time.time() - start_time\n                results[\"new_engine\"][\"avg_time\"] = results[\"new_engine\"][\"total_time\"] / num_tests\n                \n                # Test interface legacy\n                legacy_matcher = LegacySmartMatcher()\n                start_time = time.time()\n                \n                for candidate in candidates:\n                    for job in jobs:\n                        # Format simplifié pour legacy\n                        simple_candidate = {\n                            \"id\": candidate[\"id\"],\n                            \"skills\": candidate[\"skills\"]\n                        }\n                        simple_job = {\n                            \"id\": job[\"id\"],\n                            \"required_skills\": job[\"required_skills\"]\n                        }\n                        legacy_matcher.calculate_match(simple_candidate, simple_job)\n                \n                results[\"legacy_interface\"][\"total_time\"] = time.time() - start_time\n                results[\"legacy_interface\"][\"avg_time\"] = results[\"legacy_interface\"][\"total_time\"] / num_tests\n            \n            # Enregistrer les métriques\n            self.results[\"performance_metrics\"] = results\n            \n            # Afficher les résultats\n            logger.info(\"=== Résultats de performance ===\")\n            for engine_name, metrics in results.items():\n                if metrics[\"available\"]:\n                    logger.info(f\"{engine_name}: {metrics['avg_time']:.4f}s par calcul (total: {metrics['total_time']:.2f}s)\")\n                else:\n                    logger.info(f\"{engine_name}: Non disponible\")\n            \n            return True\n            \n        except Exception as e:\n            logger.error(f\"Erreur test performance: {str(e)}\")\n            return False\n    \n    async def test_insights_quality(self) -> bool:\n        \"\"\"\n        Test de la qualité des insights générés.\n        \n        Returns:\n            True si le test réussit\n        \"\"\"\n        if not NEW_ENGINE_AVAILABLE:\n            logger.warning(\"Nouveau moteur non disponible, test ignoré\")\n            return True\n        \n        try:\n            engine = SmartMatchEngine()\n            \n            # Test avec différents scénarios\n            test_cases = [\n                {\n                    \"name\": \"Excellent match\",\n                    \"candidate\": self.test_data[\"candidates\"][0],\n                    \"job\": self.test_data[\"jobs\"][0],\n                    \"expected_insights\": [\"strength\"]\n                },\n                {\n                    \"name\": \"Mismatch séniorité\",\n                    \"candidate\": self.test_data[\"candidates\"][0],\n                    \"job\": self.test_data[\"jobs\"][2],  # Job junior\n                    \"expected_insights\": [\"weakness\"]\n                }\n            ]\n            \n            success = True\n            \n            for test_case in test_cases:\n                result = await engine.calculate_match(\n                    test_case[\"candidate\"],\n                    test_case[\"job\"]\n                )\n                \n                # Vérifier que des insights sont générés\n                assert len(result.insights) > 0\n                \n                # Vérifier la structure des insights\n                for insight in result.insights:\n                    assert hasattr(insight, 'category')\n                    assert hasattr(insight, 'type')\n                    assert hasattr(insight, 'title')\n                    assert hasattr(insight, 'message')\n                    assert hasattr(insight, 'score')\n                \n                logger.info(f\"{test_case['name']}: {len(result.insights)} insights générés\")\n            \n            return success\n            \n        except Exception as e:\n            logger.error(f\"Erreur test insights: {str(e)}\")\n            return False\n    \n    async def run_all_tests(self) -> Dict[str, Any]:\n        \"\"\"\n        Exécute tous les tests et retourne un résumé des résultats.\n        \n        Returns:\n            Dict avec résultats détaillés\n        \"\"\"\n        logger.info(\"🚀 Démarrage des tests SmartMatcher Session 3\")\n        logger.info(\"=\" * 60)\n        \n        # Tests synchrones\n        self._run_test(\"Test ancien moteur - Basic\", self.test_old_engine_basic)\n        self._run_test(\"Test compatibilité legacy\", self.test_legacy_compatibility)\n        \n        # Tests asynchrones\n        await self._run_async_test(\"Test nouveau moteur - Basic\", self.test_new_engine_basic)\n        await self._run_async_test(\"Test matchers individuels\", self.test_matchers_individual)\n        await self._run_async_test(\"Test performance comparative\", self.test_performance_comparison)\n        await self._run_async_test(\"Test qualité insights\", self.test_insights_quality)\n        \n        # Calculer le taux de réussite\n        success_rate = (self.results[\"tests_passed\"] / self.results[\"tests_run\"]) * 100 if self.results[\"tests_run\"] > 0 else 0\n        \n        # Résumé final\n        logger.info(\"=\" * 60)\n        logger.info(\"📊 RÉSULTATS FINAUX\")\n        logger.info(f\"Tests exécutés: {self.results['tests_run']}\")\n        logger.info(f\"Tests réussis: {self.results['tests_passed']}\")\n        logger.info(f\"Tests échoués: {self.results['tests_failed']}\")\n        logger.info(f\"Taux de réussite: {success_rate:.1f}%\")\n        \n        if success_rate >= 80:\n            logger.info(\"✅ REFACTORING VALIDÉ avec succès !\")\n        elif success_rate >= 60:\n            logger.warning(\"⚠️  REFACTORING PARTIELLEMENT validé\")\n        else:\n            logger.error(\"❌ REFACTORING NON VALIDÉ\")\n        \n        self.results[\"success_rate\"] = success_rate\n        return self.results\n\n\nasync def main():\n    \"\"\"\n    Fonction principale pour exécuter les tests.\n    \"\"\"\n    tester = SmartMatchTester()\n    results = await tester.run_all_tests()\n    \n    # Sauvegarder les résultats si nécessaire\n    import json\n    with open(\"test_results_session3.json\", \"w\") as f:\n        # Convertir les objets non-sérialisables\n        serializable_results = {\n            \"tests_run\": results[\"tests_run\"],\n            \"tests_passed\": results[\"tests_passed\"],\n            \"tests_failed\": results[\"tests_failed\"],\n            \"success_rate\": results[\"success_rate\"],\n            \"performance_metrics\": results[\"performance_metrics\"]\n        }\n        json.dump(serializable_results, f, indent=2)\n    \n    logger.info(\"Résultats sauvegardés dans test_results_session3.json\")\n    \n    # Code de sortie basé sur le succès\n    exit_code = 0 if results[\"success_rate\"] >= 80 else 1\n    sys.exit(exit_code)\n\n\nif __name__ == \"__main__\":\n    # Vérifier les dépendances\n    if not NEW_ENGINE_AVAILABLE and not OLD_ENGINE_AVAILABLE:\n        logger.error(\"Aucun moteur SmartMatcher disponible pour les tests\")\n        sys.exit(1)\n    \n    # Exécuter les tests\n    asyncio.run(main())\n